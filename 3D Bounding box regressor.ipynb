{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib import rcParams\n",
    "from shapely.geometry import MultiPoint, box\n",
    "\n",
    "# from pyquaternion.quaternion import Quaternion\n",
    "from pyquaternion import Quaternion\n",
    "import os.path as osp\n",
    "from nuscenes import NuScenes\n",
    "\n",
    "# Utils for Lidar and Radar\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.geometry_utils import view_points, BoxVisibility\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "from nuscenes.utils.data_classes import RadarPointCloud\n",
    "from nuscenes.scripts.export_2d_annotations_as_json import get_2d_boxes, post_process_coords\n",
    "from utils import *\n",
    "\n",
    "from typing import Tuple, List, Dict, Union\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.5 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "#nusc = NuScenes(version='v1.0-mini', dataroot='/home/gus/Documents/AI_S/nuScenes/v1_0-mini', verbose=True)\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='/home/sherlock/Documents/nuScenes/data', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop to create the features and targets file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample files: 100%|██████████| 404/404 [00:00<00:00, 752.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# path with all the sample files\n",
    "samples_path =  os.getcwd() + '/dataset/samples/'\n",
    "\n",
    "# path to retrieve point clous features\n",
    "features_path =  os.getcwd() + '/dataset/point_features_path/'\n",
    "\n",
    "# Matrix to store the features' vectors\n",
    "features_matrix = []\n",
    "# Dictionary of targets\n",
    "targets_dic = {'x_pos': [], 'y_pos': [], 'z_pos': [], 'width': [], 'lenght': [], 'height': [], 'yaw': []}\n",
    "\n",
    "# extract all files in path\n",
    "for root, dirs, files in os.walk(samples_path):\n",
    "    \n",
    "    # select each sample file\n",
    "    for file_ in tqdm(files, desc = \"Sample files\"):    \n",
    "        \n",
    "        # load the current sample file\n",
    "        data_json = load_file(samples_path + file_)\n",
    "                \n",
    "        # process each instance\n",
    "        for idx in range(len(data_json['instance'])):\n",
    "            \n",
    "            # Create instance metadata\n",
    "            annotation_metadata = data_json['instance'][idx]\n",
    "\n",
    "            # Retrieve instance's features\n",
    "            features_vec = parse_features_to_numpy(annotation_metadata['point_features_path'])\n",
    "            \n",
    "            # Store the current feature vector in the features' matrix\n",
    "            features_matrix.append(list(features_vec))\n",
    "            \n",
    "            # Extract class targets\n",
    "            pose = annotation_metadata['position_coord']\n",
    "            dims = annotation_metadata['wlh_values']\n",
    "            yaw = annotation_metadata['orientation_value']\n",
    "            label = annotation_metadata['category']\n",
    "            \n",
    "            # Save X, Y and Z pose\n",
    "            targets_dic['x_pos'].append(pose[0])\n",
    "            targets_dic['y_pos'].append(pose[1])\n",
    "            targets_dic['z_pos'].append(pose[2])\n",
    "            \n",
    "            # Store target size\n",
    "            targets_dic['width'].append(dims[0])\n",
    "            targets_dic['lenght'].append(dims[1])\n",
    "            targets_dic['height'].append(dims[2])\n",
    "            \n",
    "            # Store target orientation\n",
    "            targets_dic['yaw'].append(yaw)\n",
    "\n",
    "            \n",
    "# Parse features as a numpy array\n",
    "features_matrix = np.array(features_matrix)\n",
    "\n",
    "# Create a dataframe of features\n",
    "df_features = pd.DataFrame(data = features_matrix)\n",
    "# Create a dataframe of targets\n",
    "df_targets = pd.DataFrame.from_dict(targets_dic)\n",
    "\n",
    "# Concat both dataframes horizontally to create the final dataframe\n",
    "df_detection = pd.concat([df_features, df_targets], axis=1)\n",
    "df_detection.head()\n",
    "\n",
    "df_detection.to_csv('dataset/3d_obj_detection.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing info as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Features shape is (1415, 640)\n",
      "The targets shape is (1415, 7)\n"
     ]
    }
   ],
   "source": [
    "# Parse features and targets to numpy array\n",
    "features = df_features.to_numpy()\n",
    "targets = df_targets.to_numpy()\n",
    "\n",
    "print('The Features shape is {}'.format(features.shape))\n",
    "print('The targets shape is {}'.format(targets.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set features size: \n",
      " (1132, 640)\n",
      "Test set featires size: \n",
      " (283, 640)\n",
      "Train set targets size: \n",
      " (1132, 7)\n",
      "Test set targets size: \n",
      " (283, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=1)\n",
    "\n",
    "print('Train set features size: \\n {}'.format(X_train.shape))\n",
    "print('Test set featires size: \\n {}'.format(X_test.shape))\n",
    "print('Train set targets size: \\n {}'.format(y_train.shape))\n",
    "print('Test set targets size: \\n {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) \n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = MultiOutputRegressor(Ridge(random_state = 123)).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score is: 0.61\n",
      "Test set score is: 0.18\n"
     ]
    }
   ],
   "source": [
    "print('Training set score is: {:.2f}'.format(clf.score(X_train_scaled, y_train)))\n",
    "print('Test set score is: {:.2f}'.format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = MultiOutputRegressor(svm.SVR(kernel='linear')).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Training set score is: {:.2f}'.format(clf.score(X_train_scaled, y_train)))\n",
    "print('Test set score is: {:.2f}'.format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
