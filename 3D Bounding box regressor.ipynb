{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib import rcParams\n",
    "from shapely.geometry import MultiPoint, box\n",
    "\n",
    "# from pyquaternion.quaternion import Quaternion\n",
    "from pyquaternion import Quaternion\n",
    "import os.path as osp\n",
    "from nuscenes import NuScenes\n",
    "\n",
    "# Utils for Lidar and Radar\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.geometry_utils import view_points, BoxVisibility\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "from nuscenes.utils.data_classes import RadarPointCloud\n",
    "from nuscenes.scripts.export_2d_annotations_as_json import get_2d_boxes, post_process_coords\n",
    "from utils import *\n",
    "\n",
    "from typing import Tuple, List, Dict, Union\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.5 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "#nusc = NuScenes(version='v1.0-mini', dataroot='/home/gus/Documents/AI_S/nuScenes/v1_0-mini', verbose=True)\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='/home/sherlock/Documents/nuScenes/data', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop to create the features and targets file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample files: 100%|██████████| 404/404 [00:02<00:00, 192.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# path with all the sample files\n",
    "samples_path =  os.getcwd() + '/dataset/samples/'\n",
    "\n",
    "# path to retrieve point clous features\n",
    "features_path =  os.getcwd() + '/dataset/point_features_path/'\n",
    "\n",
    "# Matrix to store the features' vectors\n",
    "features_matrix = []\n",
    "# Dictionary of targets\n",
    "targets_dic = {'x_pos': [], 'y_pos': [], 'z_pos': [], 'width': [], 'lenght': [], 'height': [], 'yaw': []}\n",
    "\n",
    "tokens_dic = {'sample_token': [], 'camera_token': [], 'category': []}\n",
    "\n",
    "# extract all files in path\n",
    "for root, dirs, files in os.walk(samples_path):\n",
    "    \n",
    "    # select each sample file\n",
    "    for file_ in tqdm(files, desc = \"Sample files\"):    \n",
    "        \n",
    "        # load the current sample file\n",
    "        data_json = load_file(samples_path + file_)\n",
    "                \n",
    "        # process each instance\n",
    "        for idx in range(len(data_json['instance'])):\n",
    "            \n",
    "            # Create instance metadata\n",
    "            annotation_metadata = data_json['instance'][idx]\n",
    "\n",
    "            # Retrieve instance's features\n",
    "            features_vec = parse_features_to_numpy(annotation_metadata['point_features_path'])\n",
    "            \n",
    "            # Store the current feature vector in the features' matrix\n",
    "            features_matrix.append(list(features_vec))\n",
    "            \n",
    "            # Extract class targets\n",
    "            pose = annotation_metadata['position_coord']\n",
    "            dims = annotation_metadata['wlh_values']\n",
    "            yaw = annotation_metadata['orientation_value']\n",
    "            label = annotation_metadata['category']\n",
    "            \n",
    "            # Save X, Y and Z pose\n",
    "            targets_dic['x_pos'].append(pose[0])\n",
    "            targets_dic['y_pos'].append(pose[1])\n",
    "            targets_dic['z_pos'].append(pose[2])\n",
    "            \n",
    "            # Store target size\n",
    "            targets_dic['width'].append(dims[0])\n",
    "            targets_dic['lenght'].append(dims[1])\n",
    "            targets_dic['height'].append(dims[2])\n",
    "            \n",
    "            # Store target orientation\n",
    "            targets_dic['yaw'].append(yaw)\n",
    "            \n",
    "            # Store sample, camera and annotation token\n",
    "            tokens_dic['sample_token'].append(annotation_metadata['sample_token'])\n",
    "            tokens_dic['camera_token'].append(annotation_metadata['camera_token'])\n",
    "            tokens_dic['category'].append(annotation_metadata['category'])\n",
    "\n",
    "            \n",
    "# Parse features as a numpy array\n",
    "features_matrix = np.array(features_matrix)\n",
    "\n",
    "# Create a dataframe of features\n",
    "df_features = pd.DataFrame(data = features_matrix)\n",
    "# Create a dataframe of targets\n",
    "df_targets = pd.DataFrame.from_dict(targets_dic)\n",
    "# Create a dataframe of tokens\n",
    "df_tokens = pd.DataFrame.from_dict(tokens_dic)\n",
    "\n",
    "# Concat both dataframes horizontally to create the final dataframe\n",
    "df_detection = pd.concat([df_features, df_targets, df_tokens], axis=1)\n",
    "df_detection.head()\n",
    "\n",
    "df_detection.to_csv('dataset/3d_obj_detection.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV and parsing info as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dataframe shape is (1415, 648)\n",
      "The target keys are Index(['x_pos', 'y_pos', 'z_pos', 'width', 'lenght', 'height', 'yaw'], dtype='object')\n",
      "The Features shape is (1415, 640)\n",
      "The targets shape is (1415, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load dataframe form CSV file\n",
    "df_detection_csv = pd.read_csv('dataset/3d_obj_detection.csv')\n",
    "# features: 1 - 640 // targets: 641 - 648\n",
    "\n",
    "print('The Dataframe shape is {}'.format(df_detection_csv.shape)) \n",
    "print('The targets keys are',df_detection_csv.keys()[641:648])\n",
    "\n",
    "# Parse features and targets to numpy array\n",
    "features = df_detection_csv.loc[:,'0':'639'].to_numpy()\n",
    "targets = df_detection_csv.loc[:,'x_pos':'yaw'].to_numpy()\n",
    "\n",
    "print('The Features shape is {}'.format(features.shape))\n",
    "print('The Targets shape is {}'.format(targets.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set features size: \n",
      " (1132, 640)\n",
      "Test set featires size: \n",
      " (283, 640)\n",
      "Train set targets size: \n",
      " (1132, 7)\n",
      "Test set targets size: \n",
      " (283, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.preprocessing import Normalizer \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=1)\n",
    "\n",
    "print('Train set features size: \\n {}'.format(X_train.shape))\n",
    "print('Test set featires size: \\n {}'.format(X_test.shape))\n",
    "print('Train set targets size: \\n {}'.format(y_train.shape))\n",
    "print('Test set targets size: \\n {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) \n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi output regressor Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = MultiOutputRegressor(Ridge(random_state = 123)).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score is: 0.61\n",
      "Test set score is: 0.18\n"
     ]
    }
   ],
   "source": [
    "print('Training set score is: {:.2f}'.format(clf.score(X_train_scaled, y_train)))\n",
    "print('Test set score is: {:.2f}'.format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train prediction:  [  1.46711326  -0.98160548  -0.39672363   2.64321882   7.41612475\n",
      "   3.20406077 -77.47530059]\n",
      "Train Targets:  [ -2.99464996  -2.70744051  -0.48460726   2.749        8.702\n",
      "   3.078      -96.66277978]\n"
     ]
    }
   ],
   "source": [
    "ridge_pred_train = clf.predict(X_train_scaled)\n",
    "print('Train prediction: ', ridge_pred_train[50])\n",
    "print(\"Train Targets: \", y_train[50] )\n",
    "# ['x_pos', 'y_pos', 'z_pos', 'width', 'lenght', 'height', 'yaw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction:  [  4.09826282   5.4450462    0.03370223   2.75658005  10.09993186\n",
      "   3.39902382 -23.9578553 ]\n",
      "Test Targets:  [ 4.74473776  9.21747484  0.26964428  2.764       9.48        3.374\n",
      " 88.13149184]\n"
     ]
    }
   ],
   "source": [
    "ridge_pred_test = clf.predict(X_test_scaled)\n",
    "print('Test prediction: ', ridge_pred_test[50])\n",
    "print(\"Test Targets: \", y_test[50] )\n",
    "# ['x_pos', 'y_pos', 'z_pos', 'width', 'lenght', 'height', 'yaw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi output regressor SVR¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score is: 0.46\n",
      "Test set score is: 0.16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = MultiOutputRegressor(svm.SVR(kernel='linear')).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Training set score is: {:.2f}'.format(clf.score(X_train_scaled, y_train)))\n",
    "print('Test set score is: {:.2f}'.format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train prediction:  [  8.57749938  -0.55795344  -0.96600857   2.03505087   4.41141789\n",
      "   1.63736505 -63.0013467 ]\n",
      "Train Targets:  [ 14.25424176  -0.45802208  -1.01023042   1.935        4.494\n",
      "   1.537      176.6700548 ]\n"
     ]
    }
   ],
   "source": [
    "svm_pred_train = clf.predict(X_train_scaled)\n",
    "print('Train prediction: ', svm_pred_train[60])\n",
    "print(\"Train Targets: \", y_train[60] )\n",
    "# ['x_pos', 'y_pos', 'z_pos', 'width', 'lenght', 'height', 'yaw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction:  [ 7.27030335  3.88958425 -0.103656    2.73236528  8.82365857  2.89078306\n",
      " 82.57343486]\n",
      "Test Targets:  [ 4.74473776  9.21747484  0.26964428  2.764       9.48        3.374\n",
      " 88.13149184]\n"
     ]
    }
   ],
   "source": [
    "svm_pred_test = clf.predict(X_test_scaled)\n",
    "print('Test prediction: ', svm_pred_test[50])\n",
    "print(\"Test Targets: \", y_test[50] )\n",
    "# ['x_pos', 'y_pos', 'z_pos', 'width', 'lenght', 'height', 'yaw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=1200,random_state=1, max_iter=800).fit(X_train_scaled, y_train)\n",
    "mlp_reg.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction:  [ -2.15200853  -1.80625099  -0.82408711   1.88459698   5.26014426\n",
      "   1.8849862  113.8675372 ]\n",
      "Test Targets:  [ 4.74473776  9.21747484  0.26964428  2.764       9.48        3.374\n",
      " 88.13149184]\n"
     ]
    }
   ],
   "source": [
    "mlp_pred_test = mlp_reg.predict(X_test_scaled)\n",
    "print('Test prediction: ', mlp_pred_test[50])\n",
    "print(\"Test Targets: \", y_test[50] )\n",
    "# ['x_pos', 'y_pos', 'z_pos', 'width', 'lenght', 'height', 'yaw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
